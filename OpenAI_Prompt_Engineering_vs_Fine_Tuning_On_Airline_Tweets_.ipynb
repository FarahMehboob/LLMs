{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFm8pVXZiMVj"
      },
      "source": [
        "# Imports\n",
        "\n",
        "Please run the cells in this section to download all packages, libraries, and dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFBR4ABzGPI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b1f7bb5-ddba-45d3-c740-704be11e2b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.1\n",
            "Collecting backoff\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: backoff\n",
            "Successfully installed backoff-2.2.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.8/238.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pygeotile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install backoff\n",
        "!pip install -q 'labelbox[data]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFccZulgFlfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc35bcf-2c1a-452b-f76d-dada8f04c815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/labelbox/schema/export_task.py:119: UserWarning: JSON converter is deprecated and will be removed in a future release\n",
            "  warnings.warn(\"JSON converter is deprecated and will be removed in a future release\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import csv\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import backoff  # for exponential backoff\n",
        "import matplotlib.pyplot as plt\n",
        "import labelbox\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from collections import Counter\n",
        "from labelbox import Client\n",
        "\n",
        "# Set the environment variable\n",
        "# Please set your own API key here\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "os.environ['LABELBOX_API_KEY'] = ''\n",
        "\n",
        "# Load your API key from an environment variable or secret management service\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "labelbox.api_key = os.getenv(\"LABELBOX_API_KEY\")\n",
        "\n",
        "client = Client(labelbox.api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmrx5l3liJFk"
      },
      "source": [
        "# ETL\n",
        "\n",
        "Please run the cells in this section to load the train and test files into dataframes and construct the randomly generated few-shot learning dataset, comprising of 100 tweet examples from the training dataset. The objective of the few-shot learning dataset (represented in the code below by the variable `selected_string`) is to pass this string as concrete examples for the LLM to learn from. This is the ultimate prompt that we will be engineering:\n",
        "\n",
        "> Given the following tweets and their corresponding airlines, separated by new lines:\n",
        "[INSERT FEW-SHOT LEARNING DATASET HERE]\n",
        "\n",
        "> Please extract the airline(s) from the following tweet:\n",
        "[INSERT TWEET HERE]\n",
        "\n",
        "> Using the following format - ['#AIRLINE_NAME_1] for one airline or ['#AIRLINE_NAME_1, #AIRLINE_NAME_2...] for multiple airlines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkZSGZlfwLFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba295a0-bd4a-457f-acc8-ad789aac69a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load your training and testing CSV files into Pandas DataFrames\n",
        "import os\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataDir = '/content/drive/My Drive//Datasets/'\n",
        "drive.mount('/content/drive')\n",
        "train = os.path.join(dataDir, 'training_data.csv')\n",
        "test = os.path.join(dataDir, 'test_data.csv')\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(train)  # Replace with the path to your training dataset\n",
        "df_test = pd.read_csv(test)    # Replace with the path to your testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_B29Ug4xzI0",
        "outputId": "b03a1f29-51c0-4d0f-b389-63d096786b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "569656873315487746 negative\n",
            "568227390566109184 neutral\n",
            "570307847281614848 positive\n",
            "567826761141985280 positive\n",
            "568140722240512001 negative\n",
            "569929677214609408 negative\n",
            "568208488335331329 negative\n",
            "570002051108769792 negative\n",
            "569990163209850881 neutral\n",
            "569161513056935936 negative\n",
            "569830069746712576 negative\n",
            "569649433635368961 negative\n",
            "567790926857531393 positive\n",
            "568989687353729024 positive\n",
            "568044610955681793 negative\n",
            "568964010101514240 negative\n",
            "567768450471587840 negative\n",
            "569268918566719488 negative\n",
            "568811854216105984 neutral\n",
            "568458513636134913 negative\n",
            "569459455273897984 negative\n",
            "569641916855402496 negative\n",
            "569791792029151232 negative\n",
            "569886288691773440 negative\n",
            "569657112109965312 negative\n",
            "568899516872568832 positive\n",
            "567735489688395776 positive\n",
            "569485181633089536 negative\n",
            "569312734896136193 positive\n",
            "570264106059624448 neutral\n",
            "569721804891271168 negative\n",
            "569602069335838720 negative\n",
            "568104264733573122 negative\n",
            "568104907384832000 neutral\n",
            "568162290890964992 positive\n",
            "568037876761546753 positive\n",
            "570181700036087809 negative\n",
            "568134534857764864 negative\n",
            "567843889748418560 negative\n",
            "569734641193586688 negative\n",
            "568458879220076545 positive\n",
            "569172065724428288 negative\n",
            "568430198254747649 negative\n",
            "569804373859020801 negative\n",
            "567754165904887810 negative\n",
            "570049623697461248 positive\n",
            "570137359787827201 negative\n",
            "569342681551400960 negative\n",
            "568495595440234496 positive\n",
            "569758145020428288 negative\n",
            "570291828349669378 neutral\n",
            "569531827989454848 negative\n",
            "568119079371997186 positive\n",
            "567756837320392705 neutral\n",
            "569950355330506752 negative\n",
            "570182980142022656 negative\n",
            "568640650129731585 positive\n",
            "569397984519016448 negative\n",
            "569709195936735232 negative\n",
            "569640998491578369 negative\n",
            "570309156290367488 negative\n",
            "569182303034974208 negative\n",
            "568850600261226496 negative\n",
            "569278644943757312 positive\n",
            "568852176799277057 positive\n",
            "569932833709527041 negative\n",
            "569903056403505152 negative\n",
            "569555455963471872 negative\n",
            "569838834810093568 negative\n",
            "569648534569361408 negative\n",
            "569551241967067136 negative\n",
            "567810259675537408 positive\n",
            "569850912451072000 negative\n",
            "570249204767055872 negative\n",
            "568444042851323904 negative\n",
            "569924965446725632 positive\n",
            "568252805036638209 negative\n",
            "567818953881419776 positive\n",
            "568833409138745344 negative\n",
            "569659317458243584 negative\n",
            "568987303302950913 negative\n",
            "568355601144680448 neutral\n",
            "568155450824216578 negative\n",
            "568450413365571584 neutral\n",
            "569975126659342336 negative\n",
            "570300248553349120 neutral\n",
            "569630593924911104 negative\n",
            "569669059941261312 negative\n",
            "569233260108124160 negative\n",
            "568056298937430016 negative\n",
            "569328072027189248 negative\n",
            "569969877286461440 positive\n",
            "569620494724366336 negative\n",
            "570300325917270018 negative\n",
            "568170405552205824 negative\n",
            "569627005852917762 negative\n",
            "567732284959113216 negative\n",
            "569670407709544448 negative\n",
            "567869456048865281 neutral\n",
            "568854401802051584 negative\n"
          ]
        }
      ],
      "source": [
        "# Read the CSV file and store rows in a list\n",
        "csv_filename = train # Replace with your CSV filename\n",
        "rows = []\n",
        "with open(csv_filename, 'r', encoding='utf-8') as csvfile:\n",
        "    csv_reader = csv.reader(csvfile)\n",
        "\n",
        "    # Skip the first row (header)\n",
        "    next(csv_reader)\n",
        "\n",
        "    for row in csv_reader:\n",
        "        if len(row) > 1:\n",
        "            text = row[0]\n",
        "            airline = row[1]\n",
        "            rows.append(f\"{text} {airline}\")\n",
        "\n",
        "# Set a seed for reproducibility (use any integer value you like)\n",
        "seed_value = 68\n",
        "random.seed(seed_value)\n",
        "\n",
        "# Shuffle the lines randomly\n",
        "random.shuffle(rows)\n",
        "\n",
        "# Select X rows from the shuffled lines. This threshold can be increased as our the context window of our chosen model increases.\n",
        "X = 100\n",
        "\n",
        "# Replace with the number of rows you want to select\n",
        "selected_rows = rows[:X]\n",
        "\n",
        "# You can also access the selected rows as a single string by joining them with newlines\n",
        "selected_string = '\\n'.join(selected_rows)\n",
        "print(selected_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xR3pt6DX9e7"
      },
      "source": [
        "# Zero-Shot Prompt Engineering Benchmark\n",
        "\n",
        "Please run the cells in this section to construct our baseline benchmark. The objective of this section is to evaluate the performance of various zero-shot prompts created via Labelbox to help us choose the best prompt for few-shot learning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the project_id represents a project from 'Humans Generate Prompts' option of the LLM Editor in Labelbox Annotate:\n",
        "project_id = \"clnlymt5y0ki2071f9g7mb3re\"\n",
        "\n",
        "project = client.get_project(project_id) # new project with skip\n",
        "\n",
        "# Extract labels (i.e prompts created by humans) in the 'DONE' stage\n",
        "filters = {\"workflow_status\": \"Done\"}\n",
        "\n",
        "export_task = project.export_v2(filters=filters)\n",
        "export_task.wait_till_done()\n",
        "\n",
        "if export_task.errors:\n",
        "  print(export_task.errors)\n",
        "\n",
        "labels = export_task.result"
      ],
      "metadata": {
        "id": "jKN0QqN_tJKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the labels list (mock structure for demonstration)\n",
        "labels = [\n",
        "    {\n",
        "        \"projects\": {\n",
        "            1: {\n",
        "                \"labels\": [\n",
        "                    {\n",
        "                        \"annotations\": {\n",
        "                            \"classifications\": [\n",
        "                                {\n",
        "                                    \"text_answer\": {\n",
        "                                        \"content\": \"What are the airlines in this tweet? '{{tweet}}'\"\n",
        "                                    }\n",
        "                                }\n",
        "                            ]\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    # Add more label entries if necessary\n",
        "]\n",
        "\n",
        "# Extract prompts from labels\n",
        "lb_prompts = []\n",
        "for label in labels:\n",
        "    for project_id in label['projects']:\n",
        "        prompt = label['projects'][project_id][\"labels\"][0][\"annotations\"][\"classifications\"][0][\"text_answer\"][\"content\"]\n",
        "        prompt = prompt.replace('\\n', '')\n",
        "        lb_prompts.append(prompt)\n",
        "\n",
        "\n",
        "\n",
        "# Format each prompt with the 'tweet' variable\n",
        "zero_shot_prompts = [f_string.format(tweet=tweet) for f_string in lb_prompts]\n",
        "\n",
        "# Print the results for verification\n",
        "print(zero_shot_prompts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks9x5ra-C9OC",
        "outputId": "a3f261ee-9aaa-4660-9948-3fb2ad30ecde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"What are the airlines in this tweet? '{tweet}'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5WX-LdFgNyv"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty DataFrame to store the evaluation metrics\n",
        "evaluation_metrics_df = pd.DataFrame(columns=['Prompt', 'Precision', 'Recall', 'F1-Score', 'Accuracy'])\n",
        "\n",
        "for prompt_text in zero_shot_prompts:\n",
        "    benchmark_results_df = pd.DataFrame(columns=['Ground Truth', 'Predictions'])\n",
        "\n",
        "    for tweet, ground_truth in zip(df_test['tweet'][:5], df_test['airlines'][:5]):\n",
        "        prompt = prompt_text.format(tweet=tweet)\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            temperature=0,\n",
        "            top_p=1,\n",
        "            max_tokens=512  # Adjust max_tokens as needed\n",
        "        )\n",
        "\n",
        "        extracted_airlines = response['choices'][0]['message']['content'].strip('\"')\n",
        "\n",
        "        # Append the results to the DataFrame\n",
        "        benchmark_results_df = benchmark_results_df.append({\n",
        "            'Ground Truth': ground_truth,\n",
        "            'Predictions': extracted_airlines\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    # Calculate evaluation metrics for this prompt\n",
        "    precision = precision_score(\n",
        "        benchmark_results_df['Ground Truth'],\n",
        "        benchmark_results_df['Predictions'],\n",
        "        average='micro'\n",
        "    )\n",
        "\n",
        "    recall = recall_score(\n",
        "        benchmark_results_df['Ground Truth'],\n",
        "        benchmark_results_df['Predictions'],\n",
        "        average='micro'\n",
        "    )\n",
        "\n",
        "    f1 = f1_score(\n",
        "        benchmark_results_df['Ground Truth'],\n",
        "        benchmark_results_df['Predictions'],\n",
        "        average='micro'\n",
        "    )\n",
        "\n",
        "    accuracy = accuracy_score(\n",
        "        benchmark_results_df['Ground Truth'],\n",
        "        benchmark_results_df['Predictions']\n",
        "    )\n",
        "\n",
        "    # Append the metrics to the evaluation DataFrame\n",
        "    evaluation_metrics_df = evaluation_metrics_df.append({\n",
        "        'Prompt': prompt_text,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'Accuracy': accuracy\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(evaluation_metrics_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metrics_df"
      ],
      "metadata": {
        "id": "Kzm26HK-LmjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9Lck2N0lxMu"
      },
      "outputs": [],
      "source": [
        "evaluation_metrics_df.to_csv('evaluation_metrics_zero_shot_prompts.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4S2RF5_wApQ"
      },
      "source": [
        "# Few-Shot Prompt Engineering with Context\n",
        "\n",
        "Please run the cells in this section to combine the few-shot dataset that we constructed in the ETL section along with the \"best\" prompt that we found in the Zero-Shot Prompt Engineering Benchmark Section. The objective of this section is to evaluate the performance of few-shot learning on the test set of tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIgjWWYlwApQ"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty DataFrame to store the ground truth and predictions from the LLM based on the few-shot prompt that was passed to it\n",
        "few_shot_prompt_engineering_results_df = pd.DataFrame(columns=['Ground Truth', 'Predictions'])\n",
        "\n",
        "for tweet, ground_truth in zip(df_test['tweet'], df_test['airlines']):\n",
        "    prompt=f\"Given the following tweets and their corresponding airlines, separated by new lines:\\n\\n{selected_string}\\n\\nPlease extract the airline(s) from the following tweet:\\n\\n{tweet}\\n\\nUsing the following format - ['#AIRLINE_NAME_1] for one airline or ['#AIRLINE_NAME_1, #AIRLINE_NAME_2...] for multiple airlines.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        # model=\"gpt-3.5-turbo-16k\",\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "          }\n",
        "        ],\n",
        "        temperature=0,\n",
        "        top_p=1,\n",
        "        max_tokens=400  # Adjust max_tokens as needed,\n",
        "    )\n",
        "\n",
        "    predicted_airlines = response['choices'][0]['message']['content'].strip('\"')\n",
        "\n",
        "    # Append the results to the DataFrame\n",
        "    few_shot_prompt_engineering_results_df = few_shot_prompt_engineering_results_df.append({\n",
        "        'Ground Truth': ground_truth,\n",
        "        'Predictions': predicted_airlines\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Save the results DataFrame to a CSV file\n",
        "few_shot_prompt_engineering_results_df.to_csv('few_shot_prompt_engineering_results_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OLSaQ8vEJEp"
      },
      "outputs": [],
      "source": [
        "# Print out our ground truth vs. predictions dataframe\n",
        "few_shot_prompt_engineering_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-hMp2h5dIuk"
      },
      "source": [
        "# [VISUALIZATIONS] - Pie Chart\n",
        "\n",
        "Please run the code cell below to visualize the distribution of the airline predictions made by the LLM on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxaDCMfMHE2X"
      },
      "outputs": [],
      "source": [
        "# Extract and flatten the 'Predictions' column\n",
        "predictions_pie_chart_fewshot = [category for category in few_shot_prompt_engineering_results_df['Predictions']]\n",
        "\n",
        "# Count the occurrences of each category\n",
        "prediction_counts_pie_chart_fewshot = dict(Counter(predictions_pie_chart_fewshot))\n",
        "\n",
        "# Sort the prediction_counts dictionary by values in descending order\n",
        "sorted_prediction_counts_piechart_fewshot = dict(sorted(prediction_counts_pie_chart_fewshot.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Extract labels and sizes\n",
        "labels = list(sorted_prediction_counts_piechart_fewshot.keys())\n",
        "sizes = list(sorted_prediction_counts_piechart_fewshot.values())\n",
        "\n",
        "# Define a threshold for explosion (e.g., 1%)\n",
        "threshold = 1.0\n",
        "\n",
        "# Calculate explosion values based on the threshold\n",
        "explode = [0.4 if (size / sum(sizes) * 100) < threshold else 0.0 for size in sizes]\n",
        "\n",
        "# Create the pie chart with explode\n",
        "plt.figure(figsize=(16, 16))  # Adjust the figure size as needed\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=200, explode=explode)\n",
        "plt.title('Distribution of Categories in Predictions')\n",
        "plt.legend(title='Categories', loc='upper left', labels=labels, ncol=2, bbox_to_anchor=(1, 1), borderaxespad=0.5)  # Set the number of columns as needed\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5nv__bmdn8b"
      },
      "source": [
        "# [VISUALIZATIONS] - Bar Chart\n",
        "\n",
        "Please run the code cell below to visualize the distribution of the airline predictions made by the LLM on the test set, juxtaposed by the ground truths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6QfeCnaS9e7"
      },
      "outputs": [],
      "source": [
        "# Sample data (replace with your own data)\n",
        "predictions = [category for category in few_shot_prompt_engineering_results_df['Predictions']]\n",
        "ground_truth = [category for category in few_shot_prompt_engineering_results_df['Ground Truth']]\n",
        "\n",
        "# Combine unique values from both predictions and ground truth\n",
        "unique_values = list(set(predictions + ground_truth))\n",
        "\n",
        "# Count the occurrences of each category for predictions and ground truth\n",
        "prediction_counts = Counter(predictions)\n",
        "ground_truth_counts = Counter(ground_truth)\n",
        "\n",
        "# Initialize lists to store counts for unique values\n",
        "unique_prediction_counts = []\n",
        "unique_ground_truth_counts = []\n",
        "\n",
        "# Populate the lists with counts, ensuring zeros for missing values\n",
        "for value in unique_values:\n",
        "    unique_prediction_counts.append(prediction_counts.get(value, 0))\n",
        "    unique_ground_truth_counts.append(ground_truth_counts.get(value, 0))\n",
        "\n",
        "# Sort the lists by descending counts\n",
        "sorted_data = sorted(zip(unique_values, unique_prediction_counts, unique_ground_truth_counts), key=lambda x: x[1], reverse=True)\n",
        "unique_values, unique_prediction_counts, unique_ground_truth_counts = zip(*sorted_data)\n",
        "\n",
        "# Set the width of the bars\n",
        "bar_width = 0.25\n",
        "\n",
        "# Create an array of indices for the x-axis\n",
        "x = np.arange(len(unique_values))\n",
        "\n",
        "# Create the figure and axis\n",
        "fig, ax = plt.subplots(figsize=(20, 12))\n",
        "\n",
        "# Plot predictions and ground truth side by side\n",
        "bar1 = ax.bar(x - bar_width / 2, unique_prediction_counts, bar_width, label='Predictions')\n",
        "bar2 = ax.bar(x + bar_width / 2, unique_ground_truth_counts, bar_width, label='Ground Truth')\n",
        "\n",
        "# Set the x-axis labels and title\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(unique_values, rotation=90)\n",
        "ax.set_xlabel('Categories')\n",
        "ax.set_ylabel('Counts')\n",
        "ax.set_title('Unique Categories Comparison between Predictions and Ground Truth')\n",
        "\n",
        "# Add a legend\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_n7FNpKncZM"
      },
      "source": [
        "# Evaluation Metrics & Misclassification Identification\n",
        "\n",
        "Please run the code cell below to calculate the precision, recall, f1-score, and accuracy for the Twitter tweets on the test set.\n",
        "\n",
        "Also used to identify the misclassifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN-MqcHQhNgD"
      },
      "outputs": [],
      "source": [
        "# Initialize empty sets for accumulating labels\n",
        "all_true_labels = set()\n",
        "all_predicted_labels = set()\n",
        "\n",
        "# Create an empty list to store misclassifications\n",
        "misclassifications = []\n",
        "\n",
        "# Iterate through each row in the DataFrame and accumulate labels\n",
        "for index, row in few_shot_prompt_engineering_results_df.iterrows():\n",
        "    true_labels = row['Ground Truth']\n",
        "    predicted_labels = row['Predictions']\n",
        "    all_true_labels.update(true_labels)\n",
        "    all_predicted_labels.update(predicted_labels)\n",
        "\n",
        "    # Check if the true and predicted labels do not match\n",
        "    if set(true_labels) != set(predicted_labels):\n",
        "        misclassifications.append(row)\n",
        "\n",
        "# Define the custom evaluation function\n",
        "def custom_evaluation(true_set, predicted_set):\n",
        "    true_set = set(true_set)\n",
        "    predicted_set = set(predicted_set)\n",
        "\n",
        "    # Precision: Intersection of true and predicted labels divided by predicted labels\n",
        "    precision = len(true_set.intersection(predicted_set)) / len(predicted_set) if len(predicted_set) > 0 else 1.0\n",
        "\n",
        "    # Recall: Intersection of true and predicted labels divided by true labels\n",
        "    recall = len(true_set.intersection(predicted_set)) / len(true_set) if len(true_set) > 0 else 1.0\n",
        "\n",
        "    # F1-score: Harmonic mean of precision and recall\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 1.0\n",
        "\n",
        "    # Accuracy: Intersection of true and predicted labels divided by the total number of labels\n",
        "    accuracy = len(true_set.intersection(predicted_set)) / len(true_set.union(predicted_set)) if len(true_set.union(predicted_set)) > 0 else 1.0\n",
        "\n",
        "    return precision, recall, f1, accuracy\n",
        "\n",
        "# Calculate custom evaluation metrics for the entire DataFrame\n",
        "precision, recall, f1, accuracy = custom_evaluation(all_true_labels, all_predicted_labels)\n",
        "\n",
        "# Print the metrics for the entire DataFrame\n",
        "print(f\"Custom Precision (Overall): {precision}\")\n",
        "print(f\"Custom Recall (Overall): {recall}\")\n",
        "print(f\"Custom F1-score (Overall): {f1}\")\n",
        "print(f\"Custom Accuracy (Overall): {accuracy}\")\n",
        "\n",
        "# Print the misclassified rows\n",
        "print(\"Misclassified Rows:\")\n",
        "for row in misclassifications:\n",
        "    print(row)\n",
        "    print('\\n')\n",
        "\n",
        "# results_df.to_csv('results_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIl1w-NnYy7z"
      },
      "source": [
        "# Fine-Tuning\n",
        "\n",
        "Please run the cells in this section to create a fine-tuned OpenAI Model, using the same 100 examples that we provided as part of our few-shot learning approach in the sections above.\n",
        "\n",
        "combine the few-shot dataset that we constructed in the ETL section along with the \"best\" prompt that we found in the **Few-Shot Prompt Engineering with Context Section**. The objective of this section is to evaluate the performance of fine-tuning on the test set of tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yexBL0AiD_sr"
      },
      "outputs": [],
      "source": [
        "# Upload your training file\n",
        "'''\n",
        "The training file should be in the following format:\n",
        "\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Given the following tweet, please extract the airline(s) from the tweet as a list of string(s).\"},{\"role\": \"user\", \"content\": \"American Air Thanks, but that results in missing the conference I'm attending. Are there options to book earlier, or if not, receive a refund?\"},{\"role\": \"assistant\", \"content\": \"['American Airlines']\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Given the following tweet, please extract the airline(s) from the tweet as a list of string(s).\"},{\"role\": \"user\", \"content\": \"@JetBlue Headphone jack not working on my flight.\"},{\"role\": \"assistant\", \"content\": \"['JetBlue Airways']\"}]}\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"Given the following tweet, please extract the airline(s) from the tweet as a list of string(s).\"},{\"role\": \"user\", \"content\": \"@JetBlue Landed at MCO before 9am and still don't have my bag. You were supposed to give it to Disney's Magical Express this AM. I am livid!\"},{\"role\": \"assistant\", \"content\": \"['JetBlue Airways']\"}]}\n",
        "...\n",
        "'''\n",
        "file_upload_response = openai.File.create(file=open(\"twitter-context.jsonl\"), purpose='fine-tune')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List uploaded files and their results\n",
        "openai.File.list()"
      ],
      "metadata": {
        "id": "cfD7AxwXAC1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = file_upload_response.id\n",
        "fine_tuned_model = openai.FineTuningJob.create(training_file=file_id, model=\"gpt-3.5-turbo-0613\")"
      ],
      "metadata": {
        "id": "x-HSVbSH-cgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List fine-tuned jobs and their results\n",
        "openai.FineTuningJob.list(limit=10)"
      ],
      "metadata": {
        "id": "tfcZSlzK9AyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Please run this cell to avoid rate limit & service unavailable errors.\n",
        "\n",
        "The function below automatically retries requests to the OpenAI servers with a random exponential backoff.\n",
        "Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request.\n",
        "If the request is still unsuccessful, the sleep length is increased and the process is repeated.\n",
        "This continues until the request is successful or until a maximum number of retries is reached.\n",
        "'''\n",
        "\n",
        "# Please see OpenAI documentation here for further details: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb\n",
        "\n",
        "@backoff.on_exception(backoff.expo, openai.error.ServiceUnavailableError)\n",
        "def completions_with_backoff(**kwargs):\n",
        "    return openai.ChatCompletion.create(**kwargs)"
      ],
      "metadata": {
        "id": "9MJA3EXNUU9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty DataFrame to store the ground truth and predictions from the LLM based on the few-shot prompt that was passed to it\n",
        "fine_tuning_results_df = pd.DataFrame(columns=['Ground Truth', 'Predictions'])\n",
        "\n",
        "for tweet, ground_truth in zip(df_test['tweet'], df_test['airlines']):\n",
        "    prompt=f\"Given the following tweet, please extract the airline(s) from the tweet as a list of string(s). {tweet}\"\n",
        "\n",
        "    response = completions_with_backoff(\n",
        "        model=\"ft:gpt-3.5-turbo-0613:personal::7xiiKWfL\",\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "          }\n",
        "        ],\n",
        "        temperature=0,\n",
        "        top_p=1,\n",
        "        max_tokens=400  # Adjust max_tokens as needed,\n",
        "    )\n",
        "\n",
        "\n",
        "    predicted_airlines = response['choices'][0]['message']['content'].strip('\"')\n",
        "\n",
        "    # Append the results to the DataFrame\n",
        "    fine_tuning_results_df = fine_tuning_results_df.append({\n",
        "        'Ground Truth': ground_truth,\n",
        "        'Predictions': predicted_airlines\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Save the results DataFrame to a CSV file\n",
        "fine_tuning_results_df.to_csv('fine_tuning_results_df.csv', index=False)"
      ],
      "metadata": {
        "id": "pQKUW2W4Kfd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuning_results_df"
      ],
      "metadata": {
        "id": "2oZPN5RsLxIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty sets for accumulating labels\n",
        "all_true_labels = set()\n",
        "all_finedtuned_predicted_labels = set()\n",
        "\n",
        "# Create an empty list to store misclassifications\n",
        "misclassifications = []\n",
        "\n",
        "# Iterate through each row in the DataFrame and accumulate labels\n",
        "for index, row in fine_tuning_results_df.iterrows():\n",
        "    true_labels = row['Ground Truth']\n",
        "    predicted_labels = row['Predictions']\n",
        "    all_true_labels.update(true_labels)\n",
        "    all_finedtuned_predicted_labels.update(predicted_labels)\n",
        "\n",
        "    # Check if the true and predicted labels do not match\n",
        "    if set(true_labels) != set(predicted_labels):\n",
        "        misclassifications.append(row)\n",
        "\n",
        "# Define the custom evaluation function\n",
        "def custom_evaluation(true_set, predicted_set):\n",
        "    true_set = set(true_set)\n",
        "    predicted_set = set(predicted_set)\n",
        "\n",
        "    # Precision: Intersection of true and predicted labels divided by predicted labels\n",
        "    precision = len(true_set.intersection(predicted_set)) / len(predicted_set) if len(predicted_set) > 0 else 1.0\n",
        "\n",
        "    # Recall: Intersection of true and predicted labels divided by true labels\n",
        "    recall = len(true_set.intersection(predicted_set)) / len(true_set) if len(true_set) > 0 else 1.0\n",
        "\n",
        "    # F1-score: Harmonic mean of precision and recall\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 1.0\n",
        "\n",
        "    # Accuracy: Intersection of true and predicted labels divided by the total number of labels\n",
        "    accuracy = len(true_set.intersection(predicted_set)) / len(true_set.union(predicted_set)) if len(true_set.union(predicted_set)) > 0 else 1.0\n",
        "\n",
        "    return precision, recall, f1, accuracy\n",
        "\n",
        "# Calculate custom evaluation metrics for the entire DataFrame\n",
        "precision, recall, f1, accuracy = custom_evaluation(all_true_labels, all_finedtuned_predicted_labels)\n",
        "\n",
        "# Print the metrics for the entire DataFrame\n",
        "print(f\"Custom Precision (Overall): {precision}\")\n",
        "print(f\"Custom Recall (Overall): {recall}\")\n",
        "print(f\"Custom F1-score (Overall): {f1}\")\n",
        "print(f\"Custom Accuracy (Overall): {accuracy}\")\n",
        "\n",
        "# Print the misclassified rows\n",
        "print(\"Misclassified Rows:\")\n",
        "for row in misclassifications:\n",
        "    print(row)\n",
        "    print('\\n')\n",
        "\n",
        "# results_df.to_csv('results_df.csv', index=False)"
      ],
      "metadata": {
        "id": "Otqnm94PtQF_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "AFm8pVXZiMVj",
        "Mmrx5l3liJFk",
        "0xR3pt6DX9e7",
        "W4S2RF5_wApQ",
        "d-hMp2h5dIuk",
        "a5nv__bmdn8b",
        "L_n7FNpKncZM",
        "EIl1w-NnYy7z"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}